# Milvus-for-ANNOY-HNSW-NSG
使用BERT和 annoy HNSW NSG 在对话中进行语义相似度搜索

# Ann
Approximate Nearest Neighbor的缩写，就是近似最近邻搜索。

在机器学习领域，语义检索，图像识别，推荐系统等方向常涉及到的一个问题是：给定一个向量X=[x1,x2,x3...xn]，需要从海量的向量库中找到最相似的前K个向量。通常这些向量的维度很高，对于在线服务，用传统的方法查找是非常耗时的，容易使得时延上成为瓶颈，因此业界通用的方式就是将最相似的查找转换成Ann问题。

这样查找返回的前K个向量并不一定是最相似的K个向量，衡量Ann算法好不好的一个依据是召回，每次Ann请求返回的K个结果与使用暴力查找的K个结果去比较，如果完全一致，说明是最好的。因为省了搜索时间却没有影响效果。

目前的Ann算法有基于图的，基于树的，基于哈希等，并且有很多关于Ann算法的实现，开源的很多，如annoy, faiss，nmslib, falconn等。


## 向量搜索工具 ANNOY HNSW NSG
### KDTree和BallTree
kd 树是一种对k维特征空间中的实例点进行存储以便对其快速检索的树形数据结构。

kd树是二叉树，核心思想是对 k 维特征空间不断切分（假设特征维度是768，对于(0,1,2,...,767)中的每一个维度，以中值递归切分）构造的树，每一个节点是一个超矩形，小于结点的样本划分到左子树，大于结点的样本划分到右子树。

树构造完毕后，最终检索时（1）从根结点出发，递归地向下访问kd树。若目标点 [公式] 当前维的坐标小于切分点的坐标，移动到左子树，否则移动到右子树，直至到达叶结点；（2）以此叶结点为“最近点”，递归地向上回退，查找该结点的兄弟结点中是否存在更近的点，若存在则更新“最近点”，否则回退；未到达根结点时继续执行（2）；（3）回退到根结点时，搜索结束。

kd树在维数小于20时效率最高，一般适用于训练实例数远大于空间维数时的k近邻搜索；当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线形扫描。

为了解决kd树在样本特征维度很高时效率低下的问题，研究人员提出了“球树“BallTree。KD 树沿坐标轴分割数据，BallTree将在一系列嵌套的超球面上分割数据，即使用超球面而不是超矩形划分区域。

具体而言，BallTree 将数据递归地划分到由质心 C 和 半径 r 定义的节点上，以使得节点内的每个点都位于由质心C和半径 r 定义的超球面内。通过使用三角不等式减少近邻搜索的候选点数。

### ANNOY
annoy全称“Approximate Nearest Neighbors Oh Yeah”，是一种适合实际应用的快速相似查找算法。Annoy 同样通过建立一个二叉树来使得每个点查找时间复杂度是O(log n)，和kd树不同的是，annoy没有对k维特征进行切分。

annoy的每一次空间划分，可以看作聚类数为2的KMeans过程。收敛后在产生的两个聚类中心连线之间建立一条垂线（图中的黑线），把数据空间划分为两部分。

在划分的子空间内不停的递归迭代继续划分，直到每个子空间最多只剩下K个数据节点，划分结束。

最终生成的二叉树底层是叶子节点记录原始数据节点，其他中间节点记录的是分割超平面的信息。

查询过程和kd树类似，先从根向叶子结点递归查找，再向上回溯即可，完整构建、查找过程可以

### 近邻图 (Proximity Graph) ：最朴素的图算法

近邻图的特性可以粗略理解成：构建一张图，每一个顶点连接着最近的 N 个顶点。它的搜索过程如下， Target 是待查询的向量。在搜索时，由于无法知道该从图的哪个区域开始搜索，所以我们选择任意一个顶点 S 出发。首先遍历 S 的邻居，找到距离与 Target 最近的 A 节点，将 A 设置为起始节点，再从 A 节点出发进行遍历，反复迭代，不断逼近，最后找到与 Target 距离最近的节点 A`` 时搜索结束。

但是基础的近邻图存在着非常多的问题，最为关键的就是搜索复杂度无法确定，孤岛效应难以解决，以及构建图的开销太高，复杂度达到了指数级别。

### HNSW
HNSW算法就是对上述朴素思想的改进和优化。为了达到快速搜索的目标，hnsw算法在构建图时还至少要满足如下要求：1）图中每个点都有“友点”；2）相近的点都互为“友点”；3）图中所有连线的数量最少；4）配有高速公路机制的构图法。

在NSW基础之上，HNSW加入了跳表结构做了进一步优化。最底层是所有数据点，每一个点都有50%概率进入上一层的有序链表。这样可以保证表层是“高速通道”，底层是精细查找。通过层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将NSW的计算复杂度由多重对数复杂度降到了对数复杂度。
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9NcWdBOFlsZ2VoN2ljWGVqQUtIMXVjN3Q3VWVKQUJOYnlZcmdXWHhEV2ZtYjlucHFTQlI1RlN0UmtXTmRjeUo0Y3ZLNDdISHltOVdQWkY4NndxZW5MSVEvNjQw?x-oss-process=image/format,png)

搜索从最上层开始，找到本层距离最近的节点之后进入下一层。下一层搜索的起始节点即是上一层的最近节点，往复循环，直至找到结果（见下图）。由于越是上层的图，节点越是稀少，平均度数也低，距离也远，所以可以通过非常小的代价提供了良好的搜索方向，通过这种方式减少大量没有价值的计算，减少了搜索算法复杂度。更进一步，如果把 HNSW 中节点的最大度数设为常数，这样可以获得一张搜索复杂度仅为 log(n) 的图。
 
HNSW 利用多层图的优势，天生保证了图的连通性。并且由于每个新节点的加入都会被随机到任意一层中，这样做一定程度上避免了由于数据输入顺序从而改变图的分布，最后影响搜索路径的情况。

### NSG

NSG 全写为 Navigating Spreading-out Graph。NSG围绕四个方向来改进： **图的连通性，减少出度，缩短搜索路径，缩减图的大小** 。具体是通过建立导航点 (Navigation Point)，特殊的选边策略,  深度遍历收回离散节点（Deep Traversal）等方法。

首先是 Navigation Point，在建图时，首先需要一张预先建立的 K-nearest-neighbor-graph (KNNG) 作为构图基准。随机选择一个点作为 Navigation Point，后续所有新插入的节点在选边时都会将Navigation Point加入候选。在建图过程中，逐渐会将子图都和 Navigation point 相连接，这样其他的节点只需保持很少的边即可，从而减少了图的大小。每次搜索从 Navigation Point 出发能够指向具体的子图，从而减少无效搜索，获得更好搜索性能。

NSG 使用的择边策略与 HNSW 类似，但是不同于 HNSW 只选择最短边为有效边，NSG 使用的择边策略如下图。以点r为例，当r与p建立连接时，以r和p为圆心，r和p的距离为半径，分别做圆，如果两个圆的交集内没有其他与p相连接的点，则r与p相连（见图3-B）。在连接点s时，由于以s和p距离为半径的交集圆内，已有点r与p相连，所以s和p不相连（见图3-C）。下图中最终与点p相连的点只有r, t 和 q（见图3-A）。

NSG 这样做的原因是考虑到由于边数一多，整个图就会变得稠密，最后在搜索时会浪费大量算力。但是减少边的数量，带来的坏处也比较明显，最后的图会是一张稀疏图，会使得有些节点难以被搜索到。不仅如此，NSG 的边还是单向边。在如此激进的策略下，图的连通性就会产生问题，这时 NSG 选择使用深度遍历来将离群的节点收回到图中。通过以上步骤，图的建立就完成了。
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9NcWdBOFlsZ2VoN2ljWGVqQUtIMXVjN3Q3VWVKQUJOYnl4aWI2d05UZGJDWjlUQU5jRGgyUUhpY1pPVFpRYlNvQmN4OWpQTWNuUUI5aWJzVWxTcHVnazdUaWF3LzY0MA?x-oss-process=image/format,png)

### 比较
HNSW 和 NSG 的比较

HNSW 从结构入手，利用分层图提高图的导航性减少无效计算从而降低搜索时间，达到优化的目的。而 NSG 选择将图的整体度数控制在尽可能小的情况下，提高导航性，缩短搜索路径来提高搜索效率。

从内存占用，搜索速度，精度等方面来比较 HNSW 和 NSG。HNSW 由于多层图的结构以及连边策略，导致搜索时内存占用量会大于 NSG，在内存受限场景下选择 NSG 会更好。但是 NSG 在建图过程中无论内存占用还是耗时都大于 HNSW。此外 HNSW 还拥有目前 NSG 不支持的特性，即增量索引，虽然耗时巨大。对比其他的索引类型，无论 NSG 还是 HNSW 在搜索时间和精度两个方面，都有巨大优势。目前在Milvus内部已经实现了 NSG 算法，并将 KNNG 计算放到了 GPU 上进行从而极大地加快了 NSG 图的构建。未来 Milvus 还会集成 HNSW，以适配更广泛的场景。

